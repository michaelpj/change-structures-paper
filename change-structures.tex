% Toggle comments for preamble and topmatter to typeset in ACM style

%\input{preamble-standard}
\input{preamble-acm}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{biblatex}[natbib=true]
\usepackage{hyperref}
\usepackage{cleveref}

\input{notation}

\newif\ifproofs
% Comment out to disable proofs
\proofstrue

\addbibresource{paper.bib}

\begin{document}

%\input{topmatter-standard}
\input{topmatter-acm}

\begin{abstract}
  Incremental computation has recently been studied using the concept of a
  \textit{derivative} of a program. This general notion allows updating programs
  based on changes in their inputs.

  We generalise the notion of derivative, and study its categorical
  properties. We develop the theory for a variety of common structures
  in computer science, including partial orders, lattices, and Boolean algebras.

  This culminates in a generic and compositional account of incremental evaluation of Datalog, as
  well as incremental update of (recursive) Datalog programs.
\end{abstract}

\title{Something about change actions}

\maketitle

\section{Introduction}

Often when we compute the value of a function, we are not only interested in the
result we get, but also in being able to (efficiently) update the result when
the input changes. 

This is the setting in which incremental computation is studied. Recently,
\textcite{cai2014changes} have given a compelling account
of \textit{change structures} and \textit{derivatives}, and used them to provide
a composable framework for incrementally evaluating lambda calculus programs.

This provides an excellent foundation for using change structures, but there are
some pressing questions:
\begin{itemize}
  \item Can we extend the concepts to cover a broader range of structures?
    The requirements for being a change structure are stringent enough that they
    prevent many useful structures in computer science from having nice change structures.
  \item Can we say anything about the abstract structure of change structures as
    a class? Being able to easily compose new change structures using standard
    constructions would be very helpful for targeting new domains.
  \item Can we perform incremental computation on fixpoints? Fixpoints are a
    common construction in programming language semantics to handle recursion,
    and so it would be desirable to be able to compute and update them incrementally.
\end{itemize}

The major motivational target for this exercise is incremental computation over
lattices and Boolean algebras. Incremental computation is especially attractive
for database or logic programming languages, where it can
produce drastically time and space savings over re-evaluation. Our particular
interest here is in Datalog, which makes heavy use of recursion, and hence needs
a good treatment of fixpoints. Moreover, an
account of incremental computation for such languages that is compatible with
the approach of \textcite{cai2014changes} opens exciting prospects for embedding
languages like Datalog seamlessly into larger incremental computations
\autocite[See][]{arntz2016datafun}.

The major contributions of this paper are:
\begin{itemize}
  \item A generalisation of change structures to \textit{change actions} and derivatives
    (\cref{sec:changeActions}).
  \item A description of the categorical properties of change actions, including
    several useful direct constructions (\cref{sec:category}).
  \item A description of the properties of change actions over 
    additional structures, including theorems that characterize the range of
    possible derivatives available, and conditions under which compound change
    actions may admit derivatives (\cref{sec:moreStructures}).
  \item Two results relating to fixpoint calculations: incremental computation
    of fixpoints, and incremental updates of fixpoint expressions (\cref{sec:fixpoints}).
  \item Generalized incremental computation and
    update of Datalog programs, with the ability to handle arbitrary additional
    language constructs, including negation and aggregation (\cref{sec:datalog}).
\end{itemize}

\section{Change actions}
\label{sec:changeActions}

\begin{defn}[Change actions]
  A \textit{change action} is defined as:

  $$\cstr{A} \defeq \cstruct{A}{\changes{A}}{\cplus}$$

  where $A$ is a set, $(\changes{A}, \splus, \mzero)$ is a monoid, and $\cplus$ gives a monoid action on $A$.

  We will call $A$ the base set, and $\changes{A}$ the change set of the change action.
\end{defn}

Elements in the change set represent changes that can be made to elements in the
base set, with the monoid action being the operation that ``applies'' the
change. The requirement that the change set be a monoid is convenient but in
fact inessential: given any set with an action on the base set, we can take the
free monoid over the action set to obtain a monoid action.

The fact that the change set is a monoid action reveals the heart of the
change action definition: the change set is a representation of a
\textit{transformation monoid} over the base set, with the monoid
operation simply being composition.

The primary motivation for change actions is that they let us define
\textit{derivatives} for functions.

\begin{defn}[Derivatives]
  A \textit{derivative} of a function $f: \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$ is a function $\derive{f}: A \times \changes{A} \rightarrow
  \changes{B}$ such that
  $$f(a \cplus \change{a}) = f(a) \cpluss \derive{f}(a, \change{a})$$

  A function which has a derivative is called \textit{differentiable}.
\end{defn}

Derivatives need not be unique, in general, so we will speak of ``a''
derivative.\footnote{In several places we will need to pick an arbitrary
  derivative for some construction. In general this needs the Axiom of Choice,
  but in most practical cases we will want to have a computable derivative
  operator for our domain, which alleviates the problem. In addition, thin
  change actions (\cref{sec:thin}) have unique derivatives.}

Derivatives capture the essence of incremental computation: given the value of a
function at a point, and a change to that point, they tell you how to compute
the new value of the function.

The choice of the name ``derivative'' is also not a coincidence. While these
derivatives do not look quite like derivatives in real analysis, they \emph{do}
bear a strong resemblance to derivatives in other areas (such as synthetic differential geometry), and
they satisfy the standard chain rule.

\begin{thm}[The Chain Rule]
  Let $f: \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$, $g: \cstr{B}_\cpluss \rightarrow \cstr{C}_\cplusss$ be differentiable functions. Then $g \circ f$ is also
  differentiable, with derivative given by
   $$\derive{(g \circ f)}(x, \change{x}) = \derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$$
   or, in curried form
   $$\derive{(g \circ f)}(x)(\change{x}) = \derive{g}(f(x)) \circ \derive{f}(x)$$
\end{thm}
\ifproofs
\begin{proof}
  By equivalence:
  \begin{itemize}
    \item[ ]$(g \circ f)(x) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$(g(f(x)) \cplusss \derive{g}\left(f(x), \derive{f}(x,\change{x})\right)$
    \item[=]$g\left(f(x) \cpluss \derive{f}(x, \change{x}) \right)$
    \item[=]$g\left(f(x \cplus \change{x})\right)$
    \item[=]$(g \circ f)(x \cplus \change{x})$
  \end{itemize}
  Therefore $\derive{g}\left(f(x), \derive{f}(x, \change{x})\right)$ is a
  derivative for $(g \circ f)$.
\end{proof}
\fi

Here are some recurring examples of changes actions:
\begin{itemize}
  \item $\cstr{A}_\discrete \defeq \cstruct{A}{\emptyset}{\emptyset}$, the discrete change action on any base set.
  \item $\cstr{A}_\Rightarrow \defeq \cstruct{A}{A\Rightarrow A}{\ev}$, where $A
    \Rightarrow A$ denotes the exponential object and $\ev$ is the evaluation map.
  \item $\cstr{A}_\leq \defeq \cstruct{A}{\leq}{ev_\leq}$, where $\leq$ is some
    transitive relation on $A$ and $ev_\leq$ denotes
    conditional application, i.e. $ev_\leq(a, (b, c))$ is equal to $c$ if $a = b$, and $a$ otherwise. Composition of changes is obtained
    by transitivity of $\leq$.
  \item $\cstr{\mathbb{Z}}_1 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{+}$
  \item $\cstr{\mathbb{Z}}_2 \defeq \cstruct{\mathbb{Z}}{\mathbb{N}}{-}$
  \item $\cstr{\mathbb{Z}}_3 \defeq \cstruct{\mathbb{Z}}{\mathbb{Z}}{+}$
  \item $\cstr{F}_2 \defeq$ integers modulo 2 with addition.
\end{itemize}

Indeed, any monoid $(A, \splus)$ can be seen as a change action $\cstruct{A}{A}{\splus}$. In particular,
for any change action $\cstruct{A}{\changes{A}}{\cplus}$,
$\cstruct{\changes{A}}{\changes{A}}{\splus}$ is also a change action. Many practical change actions
can be constructed in this way:
\begin{itemize}
  \item $[A]$, the type of lists (or streams) of elements of type $A$, is a monoid under
  concatenation. Hence it defines a change action $\cstruct{[A]}{[A]}{\doubleplus}$.
  \item $\{A\}$, the type of sets, is a monoid under either set union or intersection,
  and thus it defines change actions $\cstruct{\{A\}}{\{A\}}{\cup},\cstruct{\{A\}}{\{A\}}{\cap}$
\end{itemize}

Many other notions in computer science can be naturally understood in terms of change actions, e.g. databases
and database updates, files and \textit{diff}s, Git repositories and commits, even video compression
algorithms that encode a frame as a series of changes to the previous frame.

\subsection{Complete change actions and minus operators}

Complete change actions are an important class of change actions, because they
have changes between \emph{any} two values in the base set.

\begin{defn}[Complete change actions]
  A change action is \textit{complete} if for any $a, b \in A$, there is
  a change $\change{a} \in \changes{A}$ such that $a \cplus \change{a} = b$.
\end{defn}

Complete change actions have convenient ``minus operators'' that allow us to
compute the difference between two values.

\begin{defn}[Minus operator]
  A \textit{minus operator} is a function $\cminus: A \times A \rightarrow \changes{A}$ such that $a \cplus (b \cminus a) = b$.
\end{defn}

\begin{prop}[Completeness equivalences]
  Let $\cstr{A}$ be a change action. Then the following are equivalent:
  \begin{itemize}
    \item $\cstr{A}$ is complete.
    \item The monoid action is transitive.
    \item There is a minus operator on $\cstr{A}$.
    \item Any function from any change action into $\cstr{A}$ is differentiable.
  \end{itemize}
\end{prop}

This last property is of the utmost importance, since we are often concerned with the differentiability
of functions.

\begin{defn}[Minus derivative]
  Given a minus operator $\cminus$, we have a derivative for any function $f$,
  defined as
  $$\derive{f}_\cminus(a, \change{a}) \defeq f(a \cplus \change{a}) \cminus f(a)$$
\end{defn}

\subsection{Thin change actions}
\label{sec:thin}

As in \textcite{cai2014changes}, multiple changes may represent the difference
between two elements. This is true for many change actions, but the change
actions for which there \emph{is} only one such change are particularly
well-behaved, so it's worth naming them.

\begin{defn}[Thin change actions]
  A change action is \textit{thin} if whenever $a \cplus \change{a}
  = a \cplus \change{b}$ for some $a \in A$, it is the case that $\change{a} = \change{b}$.
\end{defn}

Many change actions are not thin, for example $\cstr{F}_2$.

\begin{prop}[Thin equivalences]
  Let $\cstr{A}$ be a change action. Then the following are equivalent.
  \begin{itemize}
    \item $\cstr{A}$ is thin.
    \item The monoid action is free.
    \item If $\cstr{A}$ has a minus operator $\cminus$, then $(a \cplus \change{a})
      \cminus a = \change{a}$.
  \end{itemize}
\end{prop}

Thinness gives us uniqueness of derivatives:

\begin{prop}
  Let $\cstr{B}$ be a thin change action, and $f: \cstr{A} \rightarrow \cstr{B}$. Then $f$ has at
  most one derivative.

  Conversely, if $\textrm{id}: \cstr{B} \rightarrow \cstr{B}$ has exactly one derivative, then
  $B$ is thin.
\end{prop}

\section{The category of change actions}
\label{sec:category}

\begin{defn}[Category of change actions]
  We define the category $\cat{CStruct}$ of change actions. The objects are
  change actions and the morphisms are differentiable functions. We denote
  the set of differentiable functions between $\cstr{A}$ and $\cstr{B}$ as $\cstr{A} \difffunc \cstr{B}$.
\end{defn}

\subsection{Equivalence with PreOrd}

There is a natural preorder on the base set of a change actions, given by reachability
 under the action:
\begin{defn}[Reachability order]
  $a \reachOrder b$ iff there is a $\change{a} \in \changes{A}$ such that $a \cplus
  \change{a} = b$.
\end{defn}

The reachability order of a complete change action on $\cstr{A}$ is precisely the trivial relation
$A \times A$. Conversely, any such change action is necessarily complete. The correspondence
between a change action and its corresponding reachability preorder gives rise to
a faithful functor $Reach : \cat{CStruct} \rightarrow \cat{PreOrd}$ that acts as the identity
on morphisms.

\begin{prop}
  A function is differentiable iff it is monotone with respect to the
  reachability order. Equivalently, the functor $Reach$ is full.
\end{prop}

\begin{corollary}
  Two change actions are isomorphic iff their posets under the reachability
  order are isomorphic.
\end{corollary}

\begin{corollary}
  Any function from a discrete change action or into a complete change
  action is differentiable.
\end{corollary}

Conversely, any preorder $\leq$ on some set $A$ induces the corresponding change action
$\cstr{A}_\leq$. This gives rise to a (full and faithful) functor $\{\_\}_\leq : \cat{PreOrd} \rightarrow \cat{CStruct}$

\begin{thm}[Equivalence to $\cat{PreOrd}$]
  The functor $Reach$ from $\cat{CStruct}$ to $\cat{PreOrd}$ together with the 
  functor $\{\_\}_\leq$ in the opposite direction form an equivalence of categories.
\end{thm}
\ifproofs
\begin{proof}
  On one hand, if $U$ is a preorder, it's trivial to check that $Reach (\{U\}_\leq) = U$.

  On the other hand, we need to find a natural isomorphism between $\{\_\}_\leq \circ Reach$
  and the identity functor. First, we note that the base set for the change action $\{Reach(A)\}$ is
  the same as the base set for $\cstr{A}$. We claim that the desired natural isomorphism is given by the
  the identity on the base sets. It remains to prove that the identities
  $id_A : \cstr{A} \rightarrow \{Reach(A)\}$ and $id_{\reachOrder} : \{Reach(A)\} \rightarrow \cstr{A}$
  is indeed differentiable in both directions.

  On one direction, a derivative is given by
  $$
    \derive{id}_A(a, \change{a}) \defeq (a, a \cplus \change{a})
  $$
  Conversely, let $(a, b) \in \reachOrder$. By definition of $\reachOrder$, there is some
  $\change{}_{(a, b)} \in \changes{A}$ satisfying $a \cplus \change{}_{(a,b)} = b$. Hence we set the
  derivative to be
  $$
    \derive{id}_{\reachOrder}(a, (a, b)) \defeq \change{}_{(a, b)}
  $$
\end{proof}
\fi

Since $\cat{PreOrd}$ is a reflective subcategory of $\cat{Set}$, this gives us a proof
of the existence of limits, colimits, and exponentials in $\cat{CStruct}$.

\begin{corollary}
  The category $\cat{CStruct}$ has all limits, colimits and exponential objects. 
\end{corollary}

\subsection{Explicit constructions}

Having shown that $\cat{CStruct}$ is equivalent to $\cat{PreOrd}$ it may seem
redundant to give explicit constructions of some of the useful categorical
notions. However, we can give constructions that are much nicer than the ones
gained by going via $\cat{PreOrd}$, which is important for using them in
practical computation.

\begin{prop}[Products]
  Let $\cstr{A} = \cstruct{A}{\changes{A}}{\cplus}$ and $\cstr{B} =
  \cstruct{B}{\changes{B}}{\cpluss}$ be change actions.

  Then $\cstr{A} \times \cstr{B} \defeq \cstruct{A \times B}{\changes{A} \times
  \changes{B}}{\cplus \times \cpluss}$ is their categorical product.
\end{prop}
\ifproofs
\begin{proof}
  Let $\cstr{Y}$ be a change action, and $f_1: \cstr{Y} \rightarrow \cstr{A}$, $f_2: \cstr{Y}
  \rightarrow \cstr{B}$ be morphisms.

  Then the product morphism in $\cat{Set}$, $\pair{f_1}{f_2}$ is the product
  morphism in $\cat{CStruct}$. It can easily be
  shown that $\pair{\derive{f_1}}{\derive{f_2}}$ is a derivative of $\pair{f_1}{f_2}$,
  hence $\pair{f_1}{f_2}$ is a morphism in $\cat{SAct}$.

  Commutativity and uniqueness follow from the corresponding properties of the
  product in the $\cat{Set}$.
\end{proof}
\fi

\begin{prop}[Coproducts]
  Let $\cstr{A} = \cstruct{A}{\changes{A}}{\cplus}$ and $\cstr{B} =
  \cstruct{B}{\changes{B}}{\cpluss}$ be change actions.

  Then $\cstr{A} + \cstr{B} \defeq \cstruct{A + B}{\changes{A} \times
  \changes{B}}{\cplusvee}$ is their categorical coproduct, with $\cplusvee$ defined as:
  $$i_1 a \cplusvee (\change{a}, \change{b}) \defeq \iota_1 (a \cplus \change{a})$$
  $$i_2 b \cplusvee (\change{a}, \change{b}) \defeq \iota_2 (b \cplus \change{b})$$
\end{prop}
\ifproofs
\begin{proof}
  Let $\cstr{Y}$ be a change action, and $f_1 : \cstr{A} \rightarrow \cstr{Y}$, $f_2 : \cstr{B}
  \rightarrow \cstr{Y}$ be differentiable functions.

  As before, it suffices to prove that the universal function $[f_1, f_2]$ in $\cat{Set}$ is a differentiable
  function from $\cstruct{A + B}{\changes{A} \times \changes{B}}{\cplusvee}$ into $Y$. It's easy to see 
  that the following morphism is a derivative:
  $$[f_1, f_2]' (i_1 a, (\change{a}, \change{b})) \defeq f_1'(a, \change{a})$$
  $$[f_1, f_2]''' (i_2 b, (\change{a}, \change{b})) \defeq f_2'(b, \change{b})$$
\end{proof}
\fi

\begin{prop}[Exponentials]
\label{prop:exponentials}
  Let $\cstr{A}_\cplus$ and $\cstr{B}_\cpluss$ be change actions

  Then $\cstruct{\cstr{A} \difffunc \cstr{B}}{A
    \rightarrow \changes{B}}{\lambda d. \lambda f. \lambda a. f(a) \cpluss
    d(a)}$ is a change action on $\cstr{A} \difffunc \cstr{B}$ and the exponential object.

  The monoid action on $A \rightarrow \changes{B}$ is the monoid
  action on $\changes{B}$ lifted pointwise, so we will typically reuse the
  change operator for $B$ for $A \rightarrow \changes{B}$.
\end{prop}
\ifproofs
\begin{proof}
  Since we are working over $\cat{Set}$, we can use the definitions of $\ev$ and
  $\curry{f}$ from $\cat{Set}$:

  $$\ev(f, a) \defeq f(a)$$
  $$\curry{f}(a) \defeq \lambda b. f((a, b))$$
  
  We merely need to show that they are differentiable: differentiability of $\curry{f}
  \times id$ follows from products; commutativity and
  uniqueness follow from the corresponding properties of the exponential object
  on $\cat{Set}$.

  Let 
  $$\derive{\ev}((f, a), (\change{f}, \change{a})) \defeq \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})$$

  Then
  \begin{itemize}
    \item[ ]$\ev((f, a) \cplus (\change{f}, \change{a}))$
    \item[=]$\ev((f \cplus \change{f}, a \cplus \change{a}))$
    \item[=]\{ definition of $\ev$ \}\\
      $(f \cplus \change{f})(a \cplus \change{a})$
    \item[=]\{ definition of $\changes{(A \rightarrow B)}$ \}\\
      $f(a \cplus \change{a}) \cplus \change{f}(a \cplus \change{a})$
    \item[=]\{ $f$ is differentiable \}\\
      $f(a) \cplus \derive{f}(a, \change{a}) \cplus \change{f}(a \cplus \change{a})$
    \item[=]\{ monoid action property \}\\
      $f(a) \cplus \left[ \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})\right]$
    \item[=]\{ definition of $\derive{\ev}$ \}\\
      $\ev(f, a) \cplus \derive{\ev}((f, a), (\change{f}, \change{a}))$
  \end{itemize}
  Therefore $\derive{\ev}$ is a derivative for $\ev$, so $\ev$ is differentiable.
  
  Let
  $$\derive{\curry{f}}(a, \change{a}) \defeq \lambda b. \derive{f}((a, b),
  (\change{a}, \mzero))$$
  
  Then
  \begin{itemize}
    \item[ ]$\curry{f}(a \cplus \change{a})$
    \item[=]\{ definition of $\curry{f}$ \}\\
      $\lambda b. f((a \cplus \change{a}, b))$
    \item[=] $\lambda b. f((a, b) \cplus (\change{a}, \mzero))$
    \item[=]\{ since $f$ is differentiable \}\\
      $\lambda b. f((a, b)) \cplus \derive{f}((a, b), (\change{a}, \mzero))$
    \item[=]\{ definition of $\changes{(A \rightarrow B)}$ \}\\
      $(\lambda b. f((a, b))) \cplus (\lambda b. \derive{f}((a, b), (\change{a}, \mzero)))$
    \item[=]\{ definition of $\derive{\curry{f}}$ \}\\
      $\curry{f}(a) \cplus \derive{\curry{f}}(a, \change{a})$
  \end{itemize}

  Therefore $\derive{\curry{f}}$ is a derivative for $\curry{f}$, so $\curry{f}$
  is differentiable.
\end{proof}
\fi

Derivatives of the evaluation map correspond to incremental evaluation of functions:

\begin{prop}[Incrementalization]
\label{prop:incrementalization}
  Let $f: \cstr{A} \rightarrow \cstr{B}$, $a \in A$, $\change{a} \in
  \changes{A}$, $\change{f} \in \changes{(A \rightarrow B)}$, and let
  $\derive{\ev}$ be a derivative of the evaluation map.

  Then 
  $$(f \cplus \change{f})(a \cplus \change{a}) = f(a) \cplus \derive{\ev}((f, a), (\change{f}, \change{a}))$$
\end{prop}

Conveniently, our proof of \cref{prop:exponentials} gives us actual derivatives for the evaluation map:

\begin{prop}[Derivatives of the evaluation map]
\label{prop:evDerivatives}
  Let $f: \cstr{A} \rightarrow \cstr{B}$, $a \in A$, $\change{a} \in
  \changes{A}$, $\change{f} \in \changes{(A \rightarrow B)}$.

  Then if $f$ is differentiable
  $$\derive{\ev}_1 \defeq \derive{f}(a, \change{a}) \splus \change{f}(a \cplus \change{a})$$

  is a derivative for the evaluation map.
  
  Or, if $f \cplus \change{f}$ is differentiable
  $$\derive{\ev}_2 \defeq \change{f}(a) \splus \derive{(f \cplus \change{f})}(a, \change{a})$$

  is a derivative for the evaluation map.
\end{prop}
\ifproofs
\begin{proof}
  The first is shown in the proof of \cref{prop:exponentials}, the second is
  similar but begins by taking the derivative of $f \cplus \change{f}$.
\end{proof}
\fi

\subsection{The category of thin change actions}

\begin{prop}
  The product $\cstr{A} \times \cstr{B}$ of change actions $\cstr{A}$ and $\cstr{B}$ is thin if and only if both $\cstr{A}$ and $\cstr{B}$ are.
  Furthermore, whenever $\cstr{A}$ and $\cstr{B}$ are complete, then so is $\cstr{A} \times \cstr{B}$.

  The exponential object $\cstr{A} \Rightarrow \cstr{B}$ of change actions $\cstr{A}$ and $\cstr{B}$ is thin if $\cstr{B}$ is. 
  Furthermore, whenever $\cstr{B}$ is complete, then so is $\cstr{A} \Rightarrow \cstr{B}$.
\end{prop}

Thus, the category of thin change actions is Cartesian closed as well.

\subsection{Ordering change actions}

We can put an order on the change actions for a given base set as follows:

\begin{defn}[Change action ordering]
  $\cstr{A}_\cplus \fineOrder \cstr{A}_\cpluss$ iff $\textrm{id}: \cstr{A}_\cplus \rightarrow \cstr{A}_\cpluss$ is differentiable.
\end{defn}

Transitivity of the order follows from the chain rule, and reflexivity is trivial.

This ordering is useful because it gives us a natural sense of the ``fineness''
of a change action, much like the corresponding version in topology.

\begin{prop}
  If $f: \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$ is differentiable, then
  \begin{itemize}
    \item if $\cstr{A}_\cplusss \fineOrder \cstr{A}_\cplus$ then $f: \cstr{A}_\cplusss \rightarrow
      \cstr{B}_\cpluss$ is differentiable.
    \item if $\cstr{B}_\cplus \fineOrder \cstr{B}_\cplusss$ then $f: \cstr{A}_\cplus \rightarrow
      \cstr{B}_\cplusss$ is differentiable.
  \end{itemize}
\end{prop}

That is, functions remain differentiable if the source change action becomes
coarser, or the target change action becomes finer (again, mirroring topology).

Furthermore, $\fineOrder$ also gives a fineness ordering on the reachability orders.

\begin{prop}
  If $a \reachOrder b$ in $\cstr{A}_\cplus$ and $\cstr{A}_\cplus \fineOrder \cstr{A}_\cpluss$, then $a \reachOrder b$ in $\cstr{A}_\cpluss$.
\end{prop}

\subsection{Superpositions}

As well as combining change actions entire, we can combine two different
change actions on the same underlying set.

\begin{defn}[Superposition]
  Let $\cstr{A}_\cplus = \cstruct{A}{\changes{A}_\cplus}{\cplus}$ and $\cstr{A}_\cpluss =
  \cstruct{A}{\changes{A}_\cpluss}{\cpluss}$ be change actions.

  Then the \textit{superposition} of $\cstr{A}_\cplus$ and $\cstr{A}_\cpluss$ is defined as:
  $$\cstr{A}_\cplus \superpose \cstr{A}_\cpluss \defeq \cstruct{A}{
    (\changes{A}_\cplus + \changes{A}_\cpluss)^\ast}{(\cplus + \cpluss)^\ast}$$
  where $X^\ast$ denotes the set of finite sequences of elements $X$.
\end{defn}

Here we have used the ``trick'' mentioned earlier to ensure that our change
set has a monoid structure: $\cstruct{A}{(\changes{A}_\cplus +
  \changes{A}_\cpluss)}{\star}$ does not have a monoid structure, so we take
the free extension to finite sequences.

In some cases we may be able to find a more compact representation of the
superposition change action, for example $\cstr{\mathbb{Z}}_1 \superpose \cstr{\mathbb{Z}}_2$ is 
isomorphic to $\cstr{\mathbb{Z}}_3$. The following result shows one way of finding such representations which
we have found helpful in practice:

\begin{prop}
  Let $\cstr{A}_\cplus$ and $\cstr{A}_\cpluss$ be two different change action on
  a set $A$. Suppose there exists a function
  $\wr : \cstr{A}_\cplus \times \cstr{A}_\cpluss \rightarrow \cstr{A}_\cplus$ satisfying for
  all $a \in A$:
  $$
    a \cplus \change{a} \cpluss \change{b} \cplus \change{c}
    = a \cplus (\change{a} + \wr(\change{c},\change{b})) \cpluss \change{b}
  $$

  Then $\cstruct{A}{\changes{A}_\cplus \times \changes{A}_\cpluss}{\cplussym{\wedge}}$
  is a change action monoidal structure given by:
  $$
    (\change{a}, \change{b}) + (\change{c}, \change{d}) \defeq 
    (\change{a} + \wr(\change{c}, \change{b}), \change{b} + \change{d})
  $$
  and action $\cplussym{\wedge}$ defined as:
  $$
    a \cplussym{\wedge} (\change{a}, \change{b}) \defeq a \cplus \change{a} \cpluss \change{b}
  $$
\end{prop}

Superposition is a useful construction, because it gives us the coarsest
change action finer than its components.

\begin{corollary}
  $\cstr{A}_\cplus \superpose \cstr{A}_\cpluss$ is the least upper bound of $\cstr{A}_\cplus$ and $\cstr{A}_\cpluss$ with respect to $\fineOrder$.
\end{corollary}
\ifproofs
\begin{proof}
  The derivative of $id_1: \cstr{A}_\cplus \rightarrow (\cstr{A}_\cplus \superpose
  \cstr{A}_\cpluss)$ is given by $\derive{id_1}(a, \change{a}) \defeq 
  i_1(\change{a})$, and similarly for $id_2$ on the other side. Hence $\cstr{A}_\cplus \superpose \cstr{A}_\cpluss$ 
  is an upper bound of $\cstr{A}_\cplus$ and $\cstr{A}_\cpluss$.

  Let $\cstr{A}_\cplusss$ be greater than $\cstr{A}_\cplus$ and $\cstr{A}_\cpluss$.
  Then a
  derivative of $id: (\cstr{A}_\cplus \superpose \cstr{A}_\cpluss) \rightarrow \cstr{A}_\cplusss$ is
  given by :
  $$
  \derive{id}(a, (\change{a_i}, \change{b_i})) \defeq 
    \sum_i (\derive{id}(a, \change{a_i}) + \derive{id}(a, \change{b_i}))
  $$
\end{proof}
\fi

This gives us a join-semilattice for change actions on a given set.

\begin{thm}[Change action semilattice]
  Change actions on a base set $A$ form a bounded join-semilattice 
  ordered by $\fineOrder$, with the least element given by
  $\cstr{A}_\discrete$, and the join operation given by $\superpose$.
\end{thm}

All functions into a complete change action are differentiable, so any
complete change actions on $A$ will be maximal elements of the lattice, while
the discrete change action provides a minimal element.

\section{Change actions over other structures}
\label{sec:moreStructures}

\subsection{Posets}

A common structure which we want to compute changes on is a poset. For this
section we shall assume that all of our base sets are posets.

\subsubsection{Approximate derivatives}

Firstly, we can define ``approximations'' to derivatives from both sides.

\begin{defn}
  Let $f: \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$ be a function. Then a \textit{sup-derivative}
  of $f$ is a function $\supderive{f}$ such that
  $$f(a \cplus \change{a}) \leq f(a) \cpluss \supderive{f}(a, \change{a})$$
  
  Similarly, a \textit{sub-derivative} of $f$ is a function $\subderive{f}$ such that 
  $$f(a \cplus \change{a}) \geq f(a) \cpluss \subderive{f}(a, \change{a})$$

  A function with a sup-derivative is sup-differentiable, and a function with a
  sub-derivative is sub-differentiable.
\end{defn}

Both sup- and sub-derivatives satisfy the chain rule, in the following sense: 
\begin{prop}[Chain rule for sub-derivatives]
  let $f : \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$ and $g : \cstr{B}_\cpluss \rightarrow \cstr{C}_\cplusss$ be
  sub-differentiable functions with sub-derivatives $f', g'$ respectively. Then
  $$\derive{g}(f(a)) \circ \derive{f}(a)$$ is a subderivative of $g \circ f$
\end{prop}

Some change actions always have sub- or sup-derivatives: for example every function
into $\cstr{\mathbb{Z}}_1$ is sup-differentiable, and every function into $\cstr{\mathbb{Z}}_2$ is 
sub-differentiable.

\begin{prop}
  If $\derive{f}$ is both a sub- and sup-derivative of $f$, then it is a derivative of $f$.
\end{prop}

Note that this is not the same as saying that if $f$ is both sub- and
sup-differentiable, then it is differentiable. The functions which provide the
sub- and sup-derivatives must coincide for that to be the case. For example,
consider any bounded lattice with the change set $\{ \lambda k . \bot, \lambda k
 . \top \}$. Then any function has a sub-derivative (mapping everything to
 bottom), and a sup-derivative (mapping everything to top), but in most cases
 these are not true derivatives.

Secondly, if the base set of a change action is a poset, then this gives us a natural
order on the change set.

\begin{defn}[Change order]
  $\change{a} \changeOrder \change{b}$ iff for all $a \in A$ it is the case that $a \cplus \change{a} \leq a \cplus \change{b}$.
\end{defn}

If the change action is thin, then the order is antisymmetric, and a
full partial order.

Having a monotone order on the changes is very useful.

\begin{thm}
  Let $f: \cstr{A} \rightarrow \cstr{B}$ be a function, and let $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotone with
  respect to it. Then let $\supderive{f}$ be a sub-derivative for $f$, and $h: A \times
  \changes{A} \rightarrow \changes{B}$ be a function such that
  $$\supderive{f} \changeOrder h$$
  Then $h$ is also a sup-derivative for $f$.

  Similarly, if $\subderive{f}$ is a sup-derivative for $f$ such that 
  $$h \changeOrder \subderive{f}$$
  Then $h$ is also a sub-derivative for $f$.
\end{thm}
\ifproofs
\begin{proof}
  We prove the first case:
  \begin{itemize}
    \item[ ]$\supderive{f}(a, \change{a}) \changeOrder h(a, \change{a})$
    \item[$\Rightarrow$]\{ by monotonicity \}\\
      $f(a) \cplus \supderive{f}(a, \change{a}) \leq f(a) \cplus h(a, \change{a})$
    \item[$\Rightarrow$]\{ sup-derivative property \}\\
      $f(a \cplus \change{a}) \leq f(a) \cplus h(a, \change{a})$
  \end{itemize}

  The proof for the other case is symmetric.
\end{proof}
\fi

\begin{thm}[Sandwich lemma]
  \label{thm:sandwich}
  Let $\supderive{f}$ be a sup-derivative for $f$, $\subderive{f}$ be a sub-derivative for $f$, $\changeOrder$ be a preorder on $\changes{B}$ such that $\cplus$ is monotone with
  respect to it, and $g$ be such that

  $$\supderive{f} \changeOrder g \changeOrder \subderive{f}$$

  Then $g$ is a derivative for $f$.
\end{thm}

In particular, this applies if $g$ and $h$ are themselves derivatives. Moreover,
although the condition of the theorem only requires the bounds to be sub- and
sup-derivatives, the conclusion of the theorem also applies to the bounds, so
they will always be full derivatives as well.

\subsubsection{Ascending and descending change actions}

Having sub- and sup-derivatives alone is not quite enough to ensure that all
functions are differentiable. We need some additional power.

\begin{defn}[Ascending and descending change actions]
  A change action $\cstr{A}$ is \textit{ascending} if $a \leq b$ implies $a
  \reachOrder b$.

  A change action $\cstr{A}$ is \textit{descending} if $a \leq b$ implies $b
  \reachOrder a$.
\end{defn}

Intuitively, an ascending change action is one where you can produce
arbitrary changes that follow the partial order.

\begin{thm}
  Let $\cstr{A}$, $\cstr{B}$ be a change actions, $f: \cstr{A} \rightarrow \cstr{B}$ be a function. Then
  any of the following are sufficient for $f$ to be differentiable.
  \begin{itemize}
    \item $\cstr{B}$ is ascending, and $f$ is sub-differentiable.
    \item $\cstr{B}$ is descending, and $f$ is sup-differentiable.
    \item $\cstr{B}$ is ascending, descending, and has a minimal or maximal element.
  \end{itemize}
\end{thm}

In particular, we can construct change actions where functions are
differentiable by superposing multiple change actions that have some of the
properties that we want.

For example, $\cstr{\mathbb{Z}}_1$ is ascending (and has sup-derivatives), and
$\cstr{\mathbb{Z}}_2$ is descending (and has sub-derivatives), so $\cstr{\mathbb{Z}}_1
\superpose \cstr{\mathbb{Z}}_2 = \cstr{\mathbb{Z}}_3$ has derivatives.

\subsubsection{Maximal and minimal derivatives}

TODO: this section isn't very elegant

If we have a minus operator, then our change action is complete and all
functions are differentiable. However, there may still be multiple derivatives
for a given function, and we can distinguish them using our order on the change
set.

\begin{defn}[Minus ordering]
  $\cminus_1 \minusOrder \cminus_2$ iff for all $a,b \in A$, $a \cminus_1 b
  \changeOrder a \cminus_2 b$.
\end{defn}

This orders our minus operators according to the size of the changes they
produce. 

\begin{prop}
  If $\cminus_1 \minusOrder \cminus_2$ then
  $\derive{f}_{\cminus_1} \changeOrder \derive{f}_{\cminus_2}$.
\end{prop}

\begin{prop}
  If $\cminus$ is a minimal (maximal) minus operator, then $\derive{f}_\cminus$
  is a minimal (maximal) derivative.
\end{prop}

This then gives us a full characterisation of the derivatives on a complete
change action.

\begin{thm}[Characterization of derivatives]
\label{thm:derivativeCharacterization}
  Let $\cstr{A}$ and $\cstr{B}$ be a change actions, let
  $f: \cstr{A} \rightarrow \cstr{B}$ be a function, and let $\subderiveM{f}$ and
  $\supderiveM{f}$ be minimal and maximal derivatives of $f$, respectively.
  Then the derivatives of $f$ are precisely
  the functions $\derive{f}$ such that
  $$\subderiveM{f} \changeOrder \derive{f} \changeOrder \supderiveM{f}$$
\end{thm}
\ifproofs
\begin{proof}
  Follows easily from \cref{thm:sandwich} and minimality/maximality.
\end{proof}
\fi

This theorem gives us leeway when trying to pick a derivative: we can pick out the
bounds, and that tells us how much ``wiggle room'' we have. This is helpful
because some of the intermediary functions may be much easier to compute than
others, as we will see in \cref{sec:datalogDifferentiability}.

\subsection{Complete partial orders}

Complete partial orders give us a notion of \emph{continuity} of functions.

\begin{defn}[Continuity]
\label{defn:continuity}
  Let $f: A \rightarrow B$ be a function, and $A$ and $B$ be a
  complete lattices. Then $f$ is \textit{continuous} if for any $D \subseteq A$,
  $$f(\bigvee_D a) = \bigvee_D f(a)$$
\end{defn}

In particular, if $\cplus$ is continuous then we can take suprema of sets of derivatives.

\begin{prop}
\label{prop:supDerivatives}
  Let $f: \cstr{A}_\cplus \rightarrow \cstr{B}_\cpluss$ be differentiable, $B$ and
  $\changes{B}$ be cpos, and suppose that $\cpluss$ is continuous
  in its second argument. Then for any set $D$ of derivatives of $f$, $\bigvee
  D$ is also a derivative.
\end{prop}
\ifproofs
\begin{proof}
  We show the derivative property directly.
  \begin{itemize}
    \item[ ]$f(a) \cpluss (\bigvee D)(f, \change{a})$
    \item[=]$f(a) \cpluss \bigvee_{\derive{f} \in D} \derive{f}(f, \change{a})$
    \item[=]\{ continuity of $\cpluss$, \cref{defn:continuity} \}\\
      $\bigvee_{\derive{f} \in D} (f(a) \cpluss \derive{f}(f, \change{a}))$
    \item[=]\{ $\derive{f}$ is a derivative \}\\
      $\bigvee_{\derive{f} \in D} f(a \cplus \change{a})$
    \item[=]\{ since $D$ is non-empty, as $f$ is differentiable \}\\
      $f(a \cplus \change{a})$
  \end{itemize}
\end{proof}
\fi

We mention another well known fact which will be useful later.

\begin{prop}
\label{prop:monotoneContinuous}
  All monotone functions on cpos which satisfy the Ascending Chain Condition
  (ACC) are continuous.
\end{prop}

\subsection{Lattices}

\begin{defn}
  Let $L$ be a join-semilattice. Then $\cstr{L}_\vee \defeq \cstruct{L}{L}{\vee}$ is a change
  action on $L$.

  Similarly, if $L$ is a meet-semilattice, then $\cstr{L}_\wedge \defeq \cstruct{L}{L}{\wedge}$ is a change
  action on $L$.
\end{defn}

\begin{prop}
  All of the following hold
  \begin{itemize}
    \item $\cstr{L}_\vee$ is ascending and has sup-derivatives.
    \item $\cstr{L}_\wedge$ is descending and has sub-derivatives.
    \item $\cstr{L}_\vee \superpose \cstr{L}_\wedge$ has derivatives.
  \end{itemize}
\end{prop}

As usual, the superposition change action is difficult to work with, since it
consists of sequences of ``upwards'' and ``downwards'' changes. However, it
makes precise the intuition that we can ``glue together'' the ``upwards'' change
action for joins and the ``downwards'' change action for meets into a
complete change action for the full lattice.

\subsection{Boolean algebras}

Boolean algebras give us a much more compact representation for the
superposition of $\cstr{L}_\vee \superpose \cstr{L}_\wedge$.

\begin{prop}
  Let $L$ be a Boolean algebra. Define
  $$\cstr{L}_\superpose \defeq \cstruct{L}{L \times L}{\twist}$$
  where
  $$a \twist (p, q) \defeq (a \vee p) \wedge \neg q$$
  and the monoid operator is
  $$(p, q) \splus (r, s) \defeq ((p \wedge \neg q) \vee r, (q \wedge \neg r) \vee s)$$

  Then $\cstr{L}_\superpose$ is isomorphic to $\cstr{L}_\vee \superpose \cstr{L}_\wedge$.
\end{prop}
\ifproofs
\begin{proof}
  We show that the monoid action property holds:
  \begin{itemize}
    \item[ ]$a \twist \left[(p, q) \splus (r, s)\right]$
    \item[=]$a \twist ((p \wedge \neg q) \vee r, (q \wedge \neg r) \vee s)$
    \item[=]$
      \left(
        a \vee
        \left(
          \left(
            p \wedge \neg q
          \right)
          \vee r
        \right)
      \right)
      \wedge \neg
      \left(
        \left(
          q \wedge \neg r
        \right)
        \vee s
      \right)$
    \item[=]\{ distributing $\vee$ over $\wedge$, applying de Morgan rules \}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \left(
            a \vee \neg q
          \right)
        \right)
        \vee r
      \right)
      \wedge 
      \left(
        \neg q \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]\{ un-distributing $\vee$ over $\wedge $\}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \left(
            a \vee \neg q
          \right)
          \wedge
          \neg q
        \right)
        \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]\{ $(A \vee B) \wedge B = B$\}\\
      $
      \left(
        \left(
          \left(  
            a \vee p
          \right)
          \wedge
          \neg q
        \right)
        \vee r
      \right)
      \wedge
      \neg s
      $
    \item[=]$a \twist (p, q) \twist (r, s)$
  \end{itemize}
\end{proof}
\fi

We can think of $\cstr{L}_\superpose$ as tracking changes as pairs of ``upwards'' and
``downwards'' changes, where the monoid action simply applies both. This
definition is much more usable than $\cstr{L}_\vee \superpose \cstr{L}_\wedge$, because we
only need to track pairs of changes, rather than sequences, which makes a big
difference in practice.

Boolean algebras also have concrete definitions for maximal and minimal minus
operators.

\begin{prop}
  Let $L$ be a Boolean algebra. Then
  $$a \cminus_\bot b = (a \wedge \neg b, b)$$
  $$a \cminus_\top b = (a, b \wedge \neg a)$$

  define minimal and maximal minus operators.
\end{prop}

In particular, \cref{thm:derivativeCharacterization} gives us bounds for
all the derivatives on Boolean algebras:

\begin{corollary}
\label{cor:booleanCharacterization}
  Let $L$ be a Boolean algebra with the $\cstr{L}_\superpose$ change action, $A$ be
  a change action, and $f: A \rightarrow
  L$ a function. Then the derivatives of $f$ are precisely those functions
  $\derive{f}$ such that
  $$
  f(a \cplus \change{a}) \cminus_\bot f(a)
  \changeOrder
  \derive{f}(a, \change{a})
  \changeOrder
  f(a \cplus \change{a}) \cminus_\top f(a)
  $$
\end{corollary}

This makes \cref{thm:derivativeCharacterization} actually usable in practice, as
we have concrete definitions for our bounds (which, again, we will make use of in \cref{sec:datalogDifferentiability}).

\section{Fixpoints}
\label{sec:fixpoints}

\subsection{Incremental computation of fixpoints}

Derivatives give us a technique for computing fixpoints incrementally. Kleene's
fixpoint theorem tells us that fixpoints exist for monotone functions on dcpos, and also gives us
a simple procedure for computing them: start from $\bot$ and apply the function
until there is no change. However, this can be woefully inefficient.

In the Datalog literature, the approach of computing the fixpoint by bottom-up
iteration is called ``naive evaluation''. Naive evaluation has the property that
if it derives a fact at some iteration, it will derive that fact at each
subsequent iteration as well. This is obviously wasteful, and can turn what
should be a linear computation into a quadratic one.

The canonical solution to this problem is ``semi-naive evaluation'', which
attempts to derive only the new facts at each iteration. However, ``semi-naive''
as traditionally presented has some warts, and
the following theorem provides a generalization of it to any differentiable function over a
change action. We will see the details of how to differentiate Datalog
semantics in \cref{sec:datalogDifferentiability}.

\begin{thm}[Incremental computation of iterated function applications]
\label{thm:diffIter}
  Let $\cstr{L}$ be a complete change action, $x \in L$, and $f: \cstr{L} \rightarrow \cstr{L}$ be differentiable. Define $F_i$ as follows:

  \begin{eqnarray*}
  F_0 & = & x\\
  \Delta F_0 & = & \mzero \\
  F_1 & = & f(F_0)\\
  \Delta F_1 & = & F_1 \cminus F_0 \\
  F_{i+2} & = & F_{i+1} \cplus \Delta F_{i+2} \\
  \Delta F_{i+2} & = & \derive{f}(F_i, \Delta F_{i+1}) \\
  \end{eqnarray*}

  Then 
  $$f^i(x) = F_i$$
\end{thm}

\ifproofs
\begin{proof}
We proceed inductively, proving that $F_{i+2} = f(F_{i+1})$

\begin{itemize}
\item[ ]$F_{i+2}$
\item[=]
$
F_{i+1} \cplus \Delta F_{i+2}
$
\item[=]
$
F_{i+1} \cplus \derive{f}( F_i, \Delta F_{i+1})
$
\item[=] \{ by induction \}\\
$
f(F_i) \cplus \derive{f}(F_i, \Delta F_{i+1})
$
\item[=]
$
f(F_i \cplus \Delta F_{i+1})
$ 
\item[=]
$f(F_{i+1})$
\end{itemize}
\end{proof}
\fi

\begin{corollary}[Differential computation of fixpoints]
\label{corollary:diffFP}
  Fixpoints of differentiable functions which can be calculated by repeated
  function application can also be calculated incrementally.
\end{corollary}

\subsection{Derivatives of fixpoints}
\label{sec:fixpointDerivatives}

The previous section has shown us how to use derivatives to compute fixpoints
more efficiently, but we might also want to take the derivative of a fixpoint
itself. The typical case for this will be where we have some fixpoint
$$\fixpoint (\lambda X . F(E, X))$$
and we now wish to apply a change to $E$ and compute
$$\fixpoint (\lambda X . F(E \cplus \change{E}, X))$$

That is, we are applying a change to the function whose fixpoint we are taking.

In Datalog this would allow us to update a recursively defined relation given an
update to a non-recursive dependency of it, such as the base database.

For example, we might want to take the archetypal transitive closure relation

$$tc(x, y) \leftarrow e(x, y)$$
$$tc(x, y) \leftarrow e(x, z), tc(z, y)$$

and update it by changing the base relation $e$.

However, this requires us to have a derivative for the fixpoint operator $\fixpoint$.

\newcommand{\theadjustment}{\operatorname{adjust}}

\begin{defn}
  $$\theadjustment(f, \change{f}) \defeq \lambda\ \change{a} . \derive{\ev}((f,
  \fixpoint_A(f), (\change{f}, \change{a})))$$
\end{defn}

\begin{thm}[Derivatives of fixpoints]
\label{thm:fixpointDiff}
  Let 
  \begin{itemize}
    \item $\cstr{A}$ be a change action
    \item$\fixpoint_A : (\cstr{A} \rightarrow \cstr{A}) \rightarrow \cstr{A}$ be a fixpoint operator
    \item $f: \cstr{A} \rightarrow \cstr{A}$ be a differentiable function
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item $\derive{\ev}$ be a derivative of the evaluation map
  \end{itemize}

  Then a change $\change{w} \in \Delta A$ satisfies
  the equation:
  \begin{equation}\label{eqn:fixcondition}
    \change{w} = \theadjustment(f, \change{f})(\change{w})
  \end{equation}
  if and only if $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$.
  
  In particular, if the operator $\fixpoint$ is defined over $\Delta A$, we can define:
  $$
  \derive{\fixpoint_A}(f, \change{f}) \defeq
  \fixpoint_{\changes{A}}(\theadjustment(f, \change{f}))
  $$
  thus $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ is a fixpoint 
  of $f \cplus \change{f}$.
\end{thm}
\ifproofs
\begin{proof}
  Let $\change{w} \in \Delta A$ satisfy \cref{eqn:fixcondition}. Then
  \begin{itemize}
  \item[ ]
    $
    (f \cplus \change{f})(\fixpoint_A(f) \cplus \change{w})
    $
  \item[=]\{ by \cref{prop:incrementalization} \}\\
    $
    f(\fixpoint_A(f))
    \cplus
    \theadjustment(f, \change{f})(\change{w})
    $
  \item[=]\{ rolling the fixpoint and \cref{eqn:fixcondition} \}\\
    $
    \fixpoint_A(f)
    \cplus
    \change{w}
    $
  \end{itemize}
  Hence $\fixpoint_A(f) \cplus \change{w}$ is a fixpoint of $f \cplus \change{f}$. The converse
  follows from reversing the direction of the proof.
\end{proof}
\fi

This is not enough to give us a true derivative, because we have only shown 
that $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ computes \emph{a} fixpoint, not necessarily
the same one computed by $\fixpoint_A{(f \cplus \change{f})}$.

However, we can get a true derivative if we are working with least fixpoints.

\begin{thm}[Derivatives of least fixpoints]
\label{thm:leastFixpointDiff}
  Let 
  \begin{itemize}
    \item $\cstr{A}$ be a change action 
    \item $A, \changes{A}$ be cpos
    \item$\fixpoint_A : (\cstr{A} \rightarrow \cstr{A}) \rightarrow \cstr{A}$ be a differentiable
      least fixpoint operator
    \item $\cplus$ be monotone with respect to the order on $\changes{A}$, and continuous
    \item $f: \cstr{A} \rightarrow \cstr{A}$ be a differentiable function
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item $\derive{\ev}$ be a derivative of the evaluation map which is monotone
      with respect to $\change{a}$
  \end{itemize}

  Then $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$ is a least
  fixpoint of $f \cplus \change{f}$ and therefore $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$.
\end{thm}
\ifproofs
\begin{proof}
  Suppose that $D$ is a derivative of $\fixpoint_A$, and hence $\fixpoint_A(f) \cplus D(f,
  \change{f})$ is the least fixpoint of $f \cplus \change{f}$. 
  Since $D$ is a derivative of $\fixpoint_A$, it follows that 
  \begin{itemize}
    \item[ ]$\fixpoint_A (f \cplus \change{f})$
    \item[=]$\fixpoint_A(f) \cplus D(f, \change{f})$
    \item[=]$\fixpoint_A(f) \cplus \theadjustment(f, \change{f})(D(f, \change{f}))$
  \end{itemize}

  That is, $\theadjustment(f, \change{f})(D(f, \change{f}))$ is also a
  derivative of $\fixpoint_A$.

  Now, let $$D_{max} \defeq \bigvee \{D \mid D \textrm{ is a derivative of }
  \fixpoint_A \}$$. This exists, since $\changes{A}$ is a complete lattice, and
  is itself a derivative by \cref{prop:supDerivatives}.

  Then, since 
  $\theadjustment(f, \change{f})(D(f, \change{f}))$ is a
  derivative, it is less than $D_{max}$. Hence $D_{max}$ is a prefix point of
  $\theadjustment(f, \change{f})$.

  Since $\derive{\ev}$ is monotone, $\theadjustment(f, \change{f})$ is also monotone, and
  hence $\fixpoint_{\changes{A}}(\theadjustment(f, \change{f}))$ is also the
  least prefix point of $\theadjustment(f, \change{f})$, and hence it is less
  than $D_{max}$.

  And so 
  \begin{itemize}
    \item[ ]
      $\fixpoint_A(f \cplus \change{f})$
    \item[$\leq$]\{ $\fixpoint_A$ is a least fixpoint \}\\
      $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f})$
    \item[$\leq$]\{ monotonicity of $\cplus$ \}\\
      $\fixpoint_A(f) \cplus D_{max}(f, \change{f})$
    \item[=]\{ $D_{max}$ is a derivative \}\\
      $\fixpoint_A(f \cplus \change{f})$
  \end{itemize}

  Therefore $\fixpoint_A(f) \cplus \derive{\fixpoint_A}(f, \change{f}) =
  \fixpoint_A(f \cplus \change{f})$, so $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$
\end{proof}
\fi

The requirements of this theorem may seem onerous. However, usually when we are
working with least fixpoints in a computational setting, we are interested in
being able to compute them by using Kleene's Theorem and iterating from $\bot$.
That means our base set is going to be a cpo satisfying the ACC, and our
functions are going to be monotone. In this setting, it's much easier to satisfy
the conditions.

\begin{prop}
  Let
  \begin{itemize}
    \item $\cstr{A}$ be a complete change action, and a cpo satisfying the ACC
    \item $\changes{A}$ be ordered according to $\changeOrder$, and a cpo
      satisfying the ACC
    \item $f: \cstr{A} \rightarrow \cstr{A}$ be differentiable and monotone
    \item $\change{f} \in \changes{(A \rightarrow A)}$ be a change to $f$
    \item$f \cplus \change{f}$ be differentiable and monotone, with a monotone derivative.
  \end{itemize}

  TODO: would be nice to have some condition which implied that $\derive{(f
    \cplus \change{f})}$ was monotone rather than having to stipulate it.

  Then
  \begin{itemize}
    \item The least fixpoint operator $\fixpoint_A$ exists for $f$ and $f \cplus
      \change{f}$ and is differentiable
    \item The least fixpoint operator $\fixpoint_{\changes{A}}$ exists for
      $\theadjustment(f, \change{f})$
    \item $\cplus$ is continuous and monotone
    \item There is a monotone derivative of $\ev$
  \end{itemize}

  And therefore $\derive{\fixpoint_A}$ is a derivative of $\fixpoint_A$.
\end{prop}
\ifproofs
\begin{proof}
  Largely trivial. Continuity of $\cplus$ follows from
  \cref{prop:monotoneContinuous}; and we take $\derive{\ev}_2$ from
  \cref{prop:evDerivatives} as our derivative of $\ev$, which is monotone 
  since $\derive{(f \cplus \change{f})}$ and $\cplus$ are monotone.
\end{proof}
\fi

The requirement that $\changes{A}$ be a cpo satisfying the ACC is still awkward, but is easily
shown to be true on Boolean algebras with $\cstr{L}_\superpose$ (so long as the base
algebra also satisfies the ACC).

Computing the derivative still requires computing a fixpoint (over the change
lattice), but this may still be significantly less expensive than the
alternative ``update strategy'': throw everything away and start
again (which will itself require a fixpoint computation).

\section{Datalog}
\label{sec:datalog}

Datalog is a well-known, simple, logic programming language. It is also a textbook
example of the need for incremental computation, as its semantics rely on
bottom-up computation of a least fixpoint \autocite[See][part D]{abiteboul1995foundations}.

There is an extensive existing literature on incremental computation of Datalog
(CITES), but the aim of this section is to argue that by viewing the computation
of Datalog semantics as composed of differentiable functions we can give a
bring the machinery that we've developed so far to bear, and give an elegant,
flexible account of incremental evaluation.

The work in this section has been applied to Semmle's commercial Datalog
implementation \autocites{semmleWebsite}{avgustinov2016ql}{sereni2008adding}{schafer2010type}.
As is it includes some non-standard features, the flexibility and composability
of the approach has been vital, since existing techniques were not applicable.

Firstly, however, we must show that we can see the semantics of Datalog in terms
of elements that we know how to handle: Boolean algebras and fixpoints.

\subsection{Denotational semantics}

Datalog is usually given a logical semantics wherein we look for models that
satisfy the program. We will instead give a simple denotational semantics that treats a Datalog
program as denoting a family of relations.

One obstacle to this approach is giving a denotation to negations. We adopt the
closed-world assumption to solve this.

\begin{defn}[Closed-world assumption and negation]
  There exists a universal relation $\universalRel$.
  
  Negation on relations is defined as $$\neg R \defeq \universalRel \setminus R$$
\end{defn}

This makes $\Rel$ into a Boolean algebra.

\begin{defn}[Term semantics]
  A Datalog term $T$ denotes a function from its free relation variables to
  $\Rel$, $\denote{\_} : \Term \rightarrow \Rel^n \rightarrow \Rel$
  \begin{eqnarray*}
    \denote{R_i}(\overline{X}) &\defeq& \overline{X}[i] \\
    \denote{T \wedge U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cap  \denote{U}(\overline{X})\\
    \denote{T \vee U}(\overline{X}) &\defeq& \denote{T}(\overline{X}) \cup  \denote{U}(\overline{X})\\
    \denote{\exists x. T}(\overline{X}) &\defeq& \pi_x(\denote{T}(\overline{X}))\\
    \denote{\neg T}(\overline{X}) &\defeq& \neg \denote{T}(\overline{X})\\
    \denote{x=y}(\overline{X}) &\defeq& \Delta(x, y) \text{ where } \Delta \text{ is the diagonal relation }
  \end{eqnarray*}
\end{defn}

Since $\Rel$ is a Boolean algebra, and so is $\Rel^n$, the denotation
functions of terms are functions between Boolean algebras.

\begin{defn}[Immediate consequence operator]
  Given a program $\mathcal{P} = \overline{P}$, the immediate consequence operator $\consq: \Rel^n \rightarrow \Rel^n$ is defined as follows:
  $$\consq(\overline{R}) = \overline{\denote{P_i}(\overline{R})}$$
\end{defn}

That is, given a value for the program, we pass in all the relations
to the denotation of each predicate, to get a new product of relations.

\begin{defn}[Program semantics]
  The semantics of a program $\mathcal{P}$ is defined to be 
  $$\denote{\mathcal{P}} \defeq \lfp_{\Rel^n}(\consq)$$
  and may be calculated by repeated application of $\consq$ to $\bot$.
\end{defn}

Whether or not this program semantics exists will depend on whether the fixpoint
exists. Typically this is ensured by constraining the program such that $\consq$
is monotone. However, we will remain agnostic on this front - we are merely
interested in calculating the semantics faster if it exists.

\subsection{Differentiability of Datalog semantics}
\label{sec:datalogDifferentiability}

In order to apply the machinery we have developed, we need the semantics $\denote{\_}$ to
be differentiable. However, it is a function on Boolean algebras, and we know
that the $\cstr{L}_\superpose$ change action is complete, so in fact we know that
$\denote{\_}$ must be differentiable.

However, this does not mean that we have a \emph{good} derivative for
$\denote{\_}$. The derivative that we know we have for complete change actions
is quite bad:
$$\derive{f}(a, \change{a}) = f(a \cplus \change{a}) \cminus f(a)$$
Naively computed, this requires \emph{more} work than evaluating $f(a \cplus \change{a})$ directly!

However, \cref{cor:booleanCharacterization} gives us a range of derivatives to
choose from, and we can optimize within that range to find one that satisfies
our constraints.

In the case of Datalog, the change ordering on the change action also
corresponds to the size of the derivative as a pair of relations. The minimal
derivative contains precisely the elements that are newly added or removed,
whereas the maximal derivative contains all the elements that have \emph{ever}
been added or removed. This means that \cref{cor:booleanCharacterization} allows
us to \emph{approximate} the most precise derivative while still being
guaranteed that the result is sound.\footnote{The idea of using an approximation
to the precise derivative, and a soundness condition, appears in \textcite{bancilhon1986amateur}.}

There is also the question of how to compute the derivative. Fortunately, the
maximal and minimal minus operators are actually representable as pairs of terms
in our Datalog, and so we can compute the derivative via a pair of terms that
satisfy those bounds, allowing us to reuse our machinery for evaluating Datalog
terms.\footnote{Indeed, if this process is occurring in an optimizing compiler,
  the derivative terms can themselves be optimized. This is likely to be
  beneficial, since the initial terms may be quite complex.}

This does give us additional constraints that the derivative terms must satisfy:
for example, we may need to pre
able to evaluate them; and we may wish to pick terms that will be easy or cheap
for our evaluation engine to evaluate, even if the results are larger.

The upshot of these considerations is that the optimal choice of derivatives is likely
to be quite dependent on the precise variant of Datalog being evaluated, and the
specifics of the evaluation engine. Here is one possibility.\footnote{These are
  the rules actually in use at Semmle. We arrived at them by starting with the
  minimal derivative and then simplifying and weakening it while preserving the
  bound given by the maximal derivative.}

\newcommand{\bothdiff}{\diamond}
\begin{thm}[Concrete Datalog term derivatives]
\label{thm:concreteDatalog}
  We give two mutually recursive definitions,
  $\updiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$ and
  $\downdiff: \Term \rightarrow \changes{\Rel^n} \rightarrow \changes{Rel}$, such
  that $\updiff \times \downdiff$ is a derivative for Datalog term semantics.
  
  Let $$\diamond X \defeq X \twist (\updiff X, \downdiff X)$$ in the following.

  TODO: make the references to the argument changes more obvious
  
  \begin{eqnarray*}
  \updiff\bot & \defeq & \bot\\
  \updiff\top & \defeq & \bot\\
  \updiff R & \defeq & \updiff R\\
  \updiff(T\vee U) & \defeq & \updiff T \vee \updiff U\\
  \updiff(T\wedge U) & \defeq & (\updiff T\wedge \bothdiff U) 
                           \vee 
                           (\updiff U \wedge \bothdiff T)\\
  \updiff(\neg T) & \defeq & \downdiff T\\
  \updiff(\exists x.T) & \defeq & \exists x.\updiff T
  \end{eqnarray*}

  \begin{eqnarray*}
  \downdiff\bot & \defeq & \bot\\
  \downdiff\top & \defeq & \bot\\
  \downdiff R & \defeq & \downdiff R\\
  \downdiff(T\vee U) & \defeq & (\downdiff T \wedge \neg \bothdiff U) 
                           \vee 
                           (\downdiff U \wedge \neg \bothdiff T)\\
  \downdiff(T\wedge U) & \defeq & (\downdiff T\wedge U) \vee (T \wedge \downdiff U)\\
  \downdiff(\neg T) & \defeq & \updiff T\\
  \downdiff(\exists x.T) & \defeq & \exists x.\downdiff T \wedge \neg \exists x.\bothdiff T
  \end{eqnarray*}
\end{thm}
\ifproofs
\begin{proof}
  Long, tedious structural induction. Maybe in an appendix?
\end{proof}
\fi

There is a symmetry between the cases for $\wedge$ and $\vee$ between $\updiff$
and $\downdiff$, but the cases for $\exists$ look quite different. 
This is because we have chosen a dialect of Datalog without a primitive universal quantifier.
If we did have one, its cases would be dual to those for $\exists$, namely:
\begin{eqnarray*}
\updiff(\forall x.T) & = & \exists x. \updiff T \wedge \forall x. \bothdiff T\\
\downdiff(\forall x.T) & = & \forall x. \downdiff T
\end{eqnarray*}

We can easily extend a derivative for the term semantics to a derivative for $\consq$.

\begin{corollary}
\label{corollary:consqDiff}
  $\consq$ is differentiable.
\end{corollary}

\subsection{Differential evaluation of Datalog}

Putting this together, we get two results.

\begin{thm}[Differential evaluation of Datalog semantics]
\label{thm:diffEval}
  Datalog program semantics can be evaluated incrementally.
\end{thm}
\ifproofs
\begin{proof}
  Corollary of \cref{corollary:diffFP} and \cref{corollary:consqDiff}.
\end{proof}
\fi

This is a generalization of semi-naive evaluation.

\begin{thm}[Differential update of Datalog semantics]
\label{thm:diffUpdate}
  Datalog program semantics can be incrementally updated with changes to non-recursive relations.
\end{thm}
\ifproofs
\begin{proof}
  Corollary of \cref{thm:leastFixpointDiff} and \cref{corollary:consqDiff}.
\end{proof}
\fi

This is a generalization of the view-update problem, including recursive relations.

\subsection{Extensions to Datalog}

Our formulation of Datalog term semantics and differential evaluation is quite
generic and composable, so it is relatively easy to extend the language with new
term constructs.

A new term construct must have:
\begin{itemize}
  \item An interpretation as a function on its free relation variables.
  \item An implementation of $\updiff$ and $\downdiff$.
\end{itemize}

These are very easy conditions - the former is needed for the construct to even
make sense, and the latter can always be satisfied by using the maximal or
minimal derivative. Although they are very bad derivatives, having them
available as options is very helpful. It means that even if we have a term
construct which we cannot find a good derivative for, we only lose incremental
evaluation for those subterms, and not for anything else.

Note that this does not say anything about monotonicity. New term constructs may
be required to be monotone if they are to participate in recursion, but we are
interested in derivatives even for non-monotone terms because they allow us to
use \cref{thm:diffUpdate}.

This is important in practice for Semmle's variant of Datalog, which includes
aggregation and other primitives with interesting derivatives.

\section{Related work}

\subsection{Change actions}

\subsubsection{Change structures}
\label{sec:relatedChangeStructures}

The seminal paper in this area is \textcite{cai2014changes}. We use the notions
defined in that excellent paper heavily, but we deviate in three regards: the
inclusion of minus operators, the nature of function changes, and the use of
dependent types.

We have omitted minus operators from our definition because
there are many interesting change actions that are not complete and so cannot 
have a minus operator (for example, $\cstr{\mathbb{Z}}_3$ does, but $\cstr{\mathbb{Z}}_1$ does
not). Where we can find a change structure with a minus operator, often we are
forced to use unwieldy representations for change sets, and
\citeauthor{cai2014changes} cite this as their reason for using a dependently
typed type of changes. For example, the change actions described before on sets and lists are clearly
useful for incremental computation on streams, yet they do not admit minus operators - instead, one would
be forced to work with e.g. multisets admitting negative arities, as \citeauthor{cai2014changes} do.

Our notion of function changes is different to \citeauthor{cai2014changes}'s,
because we consider changes of functions to be what \citeauthor{cai2014changes} describe as
\textit{pointwise differences} (for the subsequent paragraphs we shall adopt
their terminology) \autocite[See][section 2.2]{cai2014changes}. As they point out, you can reconstruct their
function changes from pointwise changes and derivatives, so the two formulations
are equivalent. This gives us the following correspondences:
\begin{itemize}
  \item Our \textit{derivatives of the
      evaluation map} are \citeauthor{cai2014changes}'s \textit{function changes}
  \item Our \textit{function changes} are \citeauthor{cai2014changes}'s \textit{pointwise differences}
\end{itemize}

Our equivalent to \citeauthor{cai2014changes}'s ``Incrementalization'' lemma
(\cref{prop:incrementalization}) is an important lemma for us as well - it is used
crucially in \cref{sec:fixpointDerivatives}. However, the incremental computation part of
the lemma simply follows from the fact that we are using the derivative of the
evaluation map - any derivative will do. It is then a further fact that we can
find actual definitions for this derivative (\cref{prop:evDerivatives}).

\citeauthor{cai2014changes} assert that the reason they use their function changes instead of pointwise
differences as the primitive notion is that a function change has access to more
information, and so is easier to optimize. In contrast, we have not found pointwise differences to be
significantly harder to work with in practice, or more difficult to compute (at least in our implementations
in Datalog).

We have two reasons to prefer using pointwise differences to function changes:
\begin{itemize}
  \item Pointwise differences are simpler from a theoretical point of view, and
    correspond to the exponentials that we get from the categorical equivalence with $\cat{PreOrd}$. 
  \item We can take pointwise differences of non-differentiable
    functions.\footnote{The fact that function changes imply differentiability
      of a function follows from \textcite[][Theorem 2.10]{cai2014changes}, and
      the fact that all change actions have a zero change.} This
    is of somewhat limited usefulness in the material we have presented, but for
    example allows us to apply \cref{thm:fixpointDiff} to non-differentiable functions.
\end{itemize}

However, this is largely an issue of presentation: if it turned out that it was
significantly easier to work with function changes rather than pointwise
changes, then there is nothing preventing a ``plugin'' for \citeauthor{cai2014changes}'s system providing the
ability to create either or both.

The equivalence of our presentations means that our work should be compatible
with ILC. In particular, this is likely to be of interest for languages like
Datafun (see \cref{sec:embeddingDatalog} below) which embed Datlog or logic programming behaviour within a broader
programming context.

Since we do not require a minus operator to always be defined, we do not need
our changes to depend on the value being changed. However,
if we want to extend our formalism to synthetic differential geometry we \emph{will} need
dependently typed changes, since the type of the tangent space at a point is
dependent on the point.

In fact, a slightly different dependently-typed formulation arises if we consider changes
to be indexed \emph{both} by the source and the target. This corresponds to 
thinking of our base set as a category, with changes $\Delta_a^b$ between $a$
and $b$ as morphisms between $a$ and $b$. In this interpretation, a function is
differentiable iff it is a functor (i.e. takes a change in $\Delta_a^b$ to one in
$\Delta_{f(a)}^{f(b)}$). Function changes also have an interpretation as natural
transformation.

This approach seems promising, although it introduces some
interesting additional restrictions (functoriality implies $\derive{f}(\change{a} \splus \change{b}) =
\derive{f}(\change{a}) \splus \derive{f}(\change{b})$; naturality of function
changes implies that
the two derivatives from \cref{prop:evDerivatives} are equal). These seem
reasonable, but we don't know exactly what the consequences would be.

\subsubsection{S-acts}

S-acts and their categorical structure have received a fair amount of attention
over the years (\textcite{kilp2000monoids} is a good
overview). However, there is a key difference between our $\cat{CStruct}$ and the category of
S-acts $\cat{SAct}$: the objects of $\cat{SAct}$ all maintain the same monoid
structure, whereas we are interested in changing both the base set \emph{and} the structure of the act.

There are similarities: if we compare the definition of a $\cat{SAct}$ ``act-preserving''
homeomorphism \autocite[See][]{kilp2000monoids} we can see that the structure is
quite similar to the definition of differentiability:

$$f(a \splus s) = f(a) \splus s$$

as opposed to

$$f(a \cplus s) = f(a) \cplus \derive{f}(a, s)$$

That is, we use $\derive{f}$ to transform the action element into the new
monoid, whereas in $\cat{SAct}$ it simply remains the same.

In fact, $\cat{SAct}$ is a subcategory of $\cat{CStruct}$, where we only
consider change actions with change set $S$, and the only functions are those
whose derivative is $\lambda a. \lambda d. d$.

\subsubsection{Derivatives of fixpoints}

\textcite{arntz2017fixpoints} gives a derivative operator for fixpoints. However,
it uses the definition of function changes from Cai et al., whereas
we have a different notion of function changes, so the result is unfortunately
inapplicable. In addition, we prove our result for all functions, not just
monotone functions.

\subsection{Datalog}

\subsubsection{Incremental evaluation}

The earliest explication of semi-naive evaluation as a derivative process
appears in \textcite{bancilhon1986naive}. The idea of using an approximate derivative
and the requisite soundness condition appears as a throwaway comment in
\textcite[][section 3.2.2]{bancilhon1986amateur}, and as far as we know nobody has since
developed that approach. 

As far as we know, traditional semi-naive is generally considered the state of
the art in incremental, bottom-up, Datalog evaluation, and there are no strategies that
accommodate additional (recursive) language features such as aggregates.

\subsubsection{Incremental updates}

There is existing literature on incremental updates to relational algebra
expressions. In particular \textcite{griffin1997improved} following
\textcite{qian1991incremental} shows the essential insight that it is necessary to
track both an ``upwards'' and a ``downwards'' difference, and produces a set of
rules that look quite similar to those we derive in \cref{thm:concreteDatalog}.

Where our presentation improves over \citeauthor{griffin1997improved} is mainly in
the genericity of the presentation. Our machinery works for a wider variety of
algebraic structures, and it is clear how the parts of the proof work together
to produce the result. In addition, it is easy to see how to extend the proofs
to cover additional language constructs.

\citeauthor{griffin1997improved} are interested in \emph{minimal} changes. These correspond to
minimal derivatives in the sense of \cref{cor:booleanCharacterization}. However,
while minimality (or proximity to minimality) is desirable (since it decreases
the size of the derivatives as relations), it is important to be able to trade
it off against other considerations. For example, at
Semmle we use the derivatives given in \cref{thm:concreteDatalog}, which are not minimal.

There are some inessential points of difference as well: we work on Datalog,
rather than relational algebra; and we use set semantics rather than bag
semantics. This is largely a matter of convenience: Datalog is an easier
language to work with, and set semantics allows a much wider range of valid
simplifications. However, all the same machinery applies to relational algebra
with bag semantics, it is simply necessary to produce a valid version of \cref{thm:concreteDatalog}.

We also solve the problem of updating \emph{recursive} expressions. As far as we
know, this is unsolved in general. Most of the attempts to solve it have
focussed on Datalog rather than relational algebra, since Datalog is designed to
make heavy use of recursion. 

Several approaches
\autocites{gupta1993maintaining}{harrison1992maintenance}
make use of a common tactic: one can get to the new fixed
point by starting from \emph{any} point below it, and then iterating the
semantics again to fixpoint. The approach, then, is to find a way to delete as
few tuples as possible to get below the new fixpoint, and then iterate again
(possibly using an incremental version of the semantics).

This is a perfectly reasonable approach, and given a good, domain-specific,
means of getting below the fixpoint, they can be quite efficient (likely more
efficient than our method). The main defect of these approaches is that they
\emph{are} domain specific, and hence inflexible with respect to changes in the
language or structure, whereas our approach is quite generic.

Other approaches \autocites{dong2000incremental}{urpi1992method} consider only
restricted subsets of Datalog, or incur other substantial constraints, and our results
are thus significantly more general.

\subsubsection{Embedding Datalog}
\label{sec:embeddingDatalog}

Datafun (\textcite{arntz2016datafun}) is a functional programming language that embeds
Datalog, allowing significant improvements in genericity, such as the use of
higher-order functions. Since we have directly defined a change action and
derivative operator for Datalog, our work could be used as a ``plugin'' in the sense
of \citeauthor{cai2014changes}, allowing Datafun to compute its internal fixpoints
incrementally, but also allowing Datafun expressions to be fully incrementally updated.

\section{Conclusions and future work}

We have presented change actions and their properties, and provided novel
strategies for incrementally evaluating fixpoints and the semantics of Datalog.

Our work opens several avenues for future investigation.

Firstly, the abstract definition of change actions could be extended to the
dependently-typed version as gestured towards in
\cref{sec:relatedChangeStructures}. We believe this might smooth over many of
the technical difficulties, and also provide a structure that is compatible with
synthetic differential geometry.

MPJ: mention the integration/integral curves stuff?

Secondly, the theory of change actions on domains could be developed. Since
domains are commonly used to define programming language semantics, this could
open up opportunities for incremental evaluation of many programming languages,
even those that not fit into the model of \citeauthor{cai2014changes}'s ILC.

Finally, combining our concrete Datalog derivatives with a system similar to ILC
in a language such as Datafun would be an exciting proof of the compositional
power of this approach.

\printbibliography

\end{document}
